{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classifier Example\n",
    "Description: Given two poems, classify the poems as either being written by Edgar Allen Poe or Rober Frost\n",
    "#### Resources \n",
    "* [Processed EAP Poems](https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt)\n",
    "* [Processed RF Poems](https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:\\\\GitHub\\\\reasearch-work\\\\natural-language-processing\\\\lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the poems\n",
    "The poems are plain text documents, so before we can build a classifier, we need to transform the documents into rows in\n",
    "a dataset. In order to do this, we must accomplish the following:\n",
    "\n",
    "* Read the plain text documents in\n",
    "* Transform each line of the poems into a row\n",
    "* Label the rows\n",
    "* Combine the rows into a single dataset\n",
    "* Split the dataset into a training set and a testing set\n",
    "\n",
    "For the purposes of the above, we select `EAP` to denote rows from an Edgar Allen Poe poem, and `RF` to denote a Robert\n",
    "Frost poem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eap_poem = utils.get_poem('https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt').split('\\n')\n",
    "rf_poem = utils.get_poem('https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt').split('\\n')\n",
    "\n",
    "eap_poem = utils.label_data(eap_poem, 0)\n",
    "rf_poem = utils.label_data(rf_poem, 1)\n",
    "\n",
    "samples = np.array(utils.flatten([eap_poem, rf_poem]))\n",
    "train_X, test_X, train_y, test_y = train_test_split(samples[:, 0], samples[:, -1], random_state=31337)\n",
    "train_y = [int(y) for y in train_y]\n",
    "test_y = [int(y) for y in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build up mapping of words\n",
    "In order to classify words, we need to index every word in the training set. We do this by tokenizing each row, and then mapping unique words to a unique index. Keep in mind we are only mapping the *training set* which means there's a statistical likelihood we'll encounter words that don't fit into our mapping when we run our classifier against the testing set. At a high-level, this means doing the following for each observation in the training set:\n",
    "\n",
    "* Clean up punctuation, and convert to lower case\n",
    "* Split each observation into individual words\n",
    "* For each word, map the word to a unique index 1..n\n",
    "* Map the key \"NOT_SEEN\" to the index 0\n",
    "* Apply the map to each row in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_helper(words):\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        row = utils.tokenize(word.rstrip())\n",
    "        tokens.append(row)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = token_helper(train_X)\n",
    "test_tokens = token_helper(test_X)\n",
    "word_map = utils.index_words(train_tokens)\n",
    "train_values = utils.word_to_idx(word_map, copy.deepcopy(train_tokens))\n",
    "test_values  = utils.word_to_idx(word_map, copy.deepcopy(test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Markov Model\n",
    "Now that we have the dataset mapped, we need to train the Markov Model. Training a Markov model involves computing the probability of a sequence given an initial state $\\pi$ and a transition matrix $A$. Here is the calculation we need to make:\n",
    "\n",
    "$log p(s_{1..T}) = log\\pi_{s_1} \\prod\\limits_{t=2}^T logA_{s_ts_{t - 1}}$\n",
    "\n",
    "We can estimate $\\pi$ as:\n",
    "\n",
    "$\\hat{\\pi} = \\frac{count(s_1 = i) + 1}{count(num\\_sequences) + N}$\n",
    "\n",
    "We can estimate $A$ as:\n",
    "\n",
    "$\\hat{A_{ij}} = \\frac{count(i \\rightarrow j) + 1}{count(i) + M}$\n",
    "\n",
    "Where $i$ is the previous state and $j$ is the state we are transitioning to. Put another way, we are evaluating the number of times we see a given sequence against the number of times we see the word by itself. Note also that we are smoothing the estimates so that we don't encounter 0 probabilities in the presence of previously unseen sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovModel():\n",
    "    def __init__(self, tokens_idx, V):\n",
    "        self.tokens = tokens_idx\n",
    "        self.V = V\n",
    "        self.A0 = np.ones((self.V, self.V))\n",
    "        self.pi0 = np.ones(self.V)\n",
    "        self.A1 = np.ones((self.V, self.V))\n",
    "        self.pi1 = np.ones(self.V)\n",
    "        self.A = None\n",
    "        self.pi = None\n",
    "        self.priors = [0., 0.]\n",
    "        self.n_classes = len(self.priors)\n",
    "    \n",
    "    def compute_counts(self, labels):        \n",
    "        for row, label in zip(self.tokens, labels):\n",
    "            prev_idx = None\n",
    "            for token in row:\n",
    "                curr_idx = token\n",
    "               \n",
    "                if not prev_idx:\n",
    "                    if label == 0:\n",
    "                        self.pi0[curr_idx] += 1\n",
    "                    else:\n",
    "                        self.pi1[curr_idx] += 1\n",
    "                else:\n",
    "                    if label == 0:\n",
    "                        self.A0[prev_idx, curr_idx] += 1\n",
    "                    else:\n",
    "                        self.A1[prev_idx, curr_idx] += 1\n",
    "                prev_idx = curr_idx\n",
    "        \n",
    "    def compute_priors(self, labels):\n",
    "        count_0 = sum(label == 0 for label in labels)\n",
    "        count_1 = sum(label == 1 for label in labels)\n",
    "        total_labels = len(labels)\n",
    "        \n",
    "        prior_0 = np.log(count_0 / total_labels)\n",
    "        prior_1 = np.log(count_1 / total_labels)\n",
    "        self.priors = [prior_0, prior_1]\n",
    "    \n",
    "    def compute_log_likelihood(self, observation, class_label):\n",
    "        A = self.A[class_label]\n",
    "        pi = self.pi[class_label]\n",
    "        \n",
    "        prev_idx = None\n",
    "        prob = 0.\n",
    "        \n",
    "        for idx in observation:\n",
    "            if not prev_idx:\n",
    "                prob += pi[idx]\n",
    "                continue\n",
    "            prob += A[prev_idx, idx]\n",
    "            prev_idx = idx\n",
    "        return prob\n",
    "    \n",
    "    def normalize_pi(self, pi):\n",
    "        return np.log(pi / pi.sum())\n",
    "    \n",
    "    def normalize_A(self, A):\n",
    "        return np.log(A / A.sum(axis=1, keepdims=True))\n",
    "    \n",
    "    def predict(self, observations):\n",
    "        self.A = [self.A0, self.A1]\n",
    "        self.pi = [self.pi0, self.pi1]\n",
    "        predictions = np.zeros(len(observations))\n",
    "        \n",
    "        for i, observation in enumerate(observations):\n",
    "            posteriors = [self.compute_log_likelihood(observation, class_label) \\\n",
    "                          + self.priors[class_label] for class_label in range(self.n_classes)]\n",
    "            prediction = np.argmax(posteriors)\n",
    "            predictions[i] = prediction\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MarkovModel(train_values, len(word_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_counts(train_y)\n",
    "model.pi0 = model.normalize_pi(model.pi0)\n",
    "model.pi1 = model.normalize_pi(model.pi1)\n",
    "model.A0 = model.normalize_A(model.A0)\n",
    "model.A1 = model.normalize_A(model.A1)\n",
    "model.compute_priors(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_ACCURACY:  0.74984520123839\n"
     ]
    }
   ],
   "source": [
    "p_train = model.predict(train_values)\n",
    "print('TRAIN_ACCURACY: ', np.mean(p_train == train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_ACCURACY:  0.7087198515769945\n"
     ]
    }
   ],
   "source": [
    "p_test = model.predict(test_values)\n",
    "print('TEST_ACCURACY: ', np.mean(p_test == test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 174,  352],\n",
       "       [  52, 1037]], dtype=int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(train_y, p_train)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 53, 139],\n",
       "       [ 18, 329]], dtype=int64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(test_y, p_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8369652945924132"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(train_y, p_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.807361963190184"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y, p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
